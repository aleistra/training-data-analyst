{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying and Making Predictions with a Trained Model\n",
    "\n",
    "**Learning Objectives**\n",
    "- Deploy a model on Google CMLE\n",
    "- Make online and batch predictions with a deployed model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will deploy the model we trained to predict birthweight and we will use that deployed model to make predictions using our cloud-hosted machine learning model. Cloud ML Engine provides two ways to get predictions from trained models; i.e., online prediction and batch prediction; and we do both in this notebook. \n",
    "\n",
    "Have a look at this blog post on [Online vs Batch Prediction](https://cloud.google.com/ml-engine/docs/tensorflow/online-vs-batch-prediction) to see the trade-offs of both approaches.\n",
    "\n",
    "As usual we start by setting our environment variables to reference our Project and Bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-636667ae83e902b6\"  # Replace with your PROJECT\n",
    "BUCKET =  \"qwiklabs-gcp-636667ae83e902b6_al\"  # Replace with your BUCKET\n",
    "REGION = \"us-east1\"            # Choose an available region for AI Platform  \n",
    "TFVERSION = \"1.13\"                # TF version for AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls -r gs://${BUCKET} | grep -q gs://${BUCKET}/babyweight/trained_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls -r gs://${BUCKET} | grep -q gs://${BUCKET}/babyweight/trained_model/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    # copy canonical model if you didn't do previous notebook\n",
    "    gsutil -m cp -R gs://cloud-training-demos/babyweight/trained_model gs://${BUCKET}/babyweight/trained_model\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy trained model\n",
    "\n",
    "Next we'll deploy the trained model to act as a REST web service using a simple gcloud call. To start, we'll check if our model and version already exists and if so, we'll delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model 'babyweight' with version 'ml_on_gcp' does not exist.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"ml_on_gcp\"\n",
    "\n",
    "# Check to see if the model and version already exist, \n",
    "# if so, delete them to deploy anew\n",
    "if gcloud ai-platform models list | grep \"$MODEL_NAME \\+ $MODEL_VERSION\"; then\n",
    "    echo \"Deleting the version '$MODEL_VERSION' of model '$MODEL_NAME'\"\n",
    "    gcloud ai-platform versions delete ${MODEL_VERSION} --model=$MODEL_NAME\n",
    "    echo \"Deleting the model '$MODEL_NAME'\"\n",
    "    gcloud ai-platform models delete ${MODEL_NAME}\n",
    "else \n",
    "    echo \"The model '$MODEL_NAME' with version '$MODEL_VERSION' does not exist.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now deploy our model. This will take a few minutes. Once the cell below completes, you should be able to see your newly deployed model in the 'Models' portion of the[ ML Engine section of the GCP console](https://pantheon.corp.google.com/mlengine/models).\n",
    "\n",
    "Let's have a look at the contents of the `exporter` bucket to see which model binaries we have. We can deploy a model by specifying any of these locations. To make sure we grab the model trained from the most recent training job, we'll use `tail -1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/1563904486/\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/1563905082/\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/1563905980/\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/1563906363/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "After completing the TODOs in the code cell below, we will be able to deploy our saved model to the cloud and make predictions. There are two TODOs below. \n",
    "- For the first TODO, write a `gcloud` command to create a model called `babyweight`. \n",
    "- In the second TODO, write a `gcloud` to create a version called `ml_on_gcp`.\n",
    "\n",
    "Look up the Cloud ML-Engine documentation to remind your self how to create these commands. You'll need to provide use the `MODEL_NAME`, `MODEL_VERSION`, `MODEL_LOCATION`, `REGION` and `TFVERSION` provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"ml_on_gcp\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/export/exporter/ | tail -1)\n",
    "\n",
    "echo $MODEL_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying the model 'babyweight', version 'ml_on_gcp_v3' from 'gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model/export/exporter/1563906363/' ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_VERSION=\"ml_on_gcp_v3\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deploying the model '$MODEL_NAME', version '$MODEL_VERSION' from '$MODEL_LOCATION' ... this will take a few minutes\"\n",
    "#gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin $MODEL_LOCATION --runtime-version $TFVERSION --python-version=3.5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the deployed model to make online predictions\n",
    "\n",
    "To make online predictions, we'll send a JSON request to the endpoint of the service to make it predict a baby's weight. The order of the responses are the order of the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 2**\n",
    "\n",
    "In the cell below we'll make online predictions with the model we just deployed. In order to do that, we need to set up the right `token` and `api` to create the correct request post at the bottom. Complete the TODOs below. You will need to \n",
    "- Specify the correct `MODEL_NAME` and `MODEL_VERSION` we want to use for prediction\n",
    "- Use `GoogleCredentials` library to create an access token\n",
    "- Create a variable called `api` which specifies the Google prediciton API using the Project, model name, and model version\n",
    "- Add an addtional instance to prediction with the following properties\n",
    "      'is_male': 'Unknown',\n",
    "      'mother_age': 29.0,\n",
    "      'plurality': 'Multiple(2+)',\n",
    "      'gestation_weeks': 38\n",
    "- Create a variable called `response` which will post a request to our model API to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response = {'predictions': [{'predictions': [7.6306610107421875]}, {'predictions': [7.043222904205322]}, {'predictions': [4.760789394378662]}, {'predictions': [6.05697774887085]}]}\n"
     ]
    }
   ],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "import json\n",
    "from googleapiclient import discovery\n",
    "\n",
    "MODEL_NAME = \"babyweight\"\n",
    "MODEL_VERSION = \"ml_on_gcp_v3\"\n",
    "\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build(\"ml\", \"v1\", credentials = credentials,\n",
    "            discoveryServiceUrl = \"https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json\")\n",
    "\n",
    "\n",
    "data = {\n",
    "  \"instances\": [\n",
    "    {\n",
    "      \"is_male\": \"True\",\n",
    "      \"mother_age\": 26.0,\n",
    "      \"plurality\": \"Single(1)\",\n",
    "      \"gestation_weeks\": 39,\n",
    "      \"mother_race\": \"1\",\n",
    "      \"father_race\": \"1\",\n",
    "      \"cigarette_use\": \"False\",\n",
    "      \"mother_married\": \"True\",\n",
    "      \"ever_born\": \"1\"  \n",
    "    },\n",
    "    {\n",
    "      \"is_male\": \"False\",\n",
    "      \"mother_age\": 29.0,\n",
    "      \"plurality\": \"Single(1)\",\n",
    "      \"gestation_weeks\": 38,\n",
    "      \"mother_race\": \"1\",\n",
    "      \"father_race\": \"1\",\n",
    "      \"cigarette_use\": \"False\",\n",
    "      \"mother_married\": \"True\",\n",
    "      \"ever_born\": \"1\"  \n",
    "    },\n",
    "    {\n",
    "      \"is_male\": \"True\",\n",
    "      \"mother_age\": 26.0,\n",
    "      \"plurality\": \"Triplets(3)\",\n",
    "      \"gestation_weeks\": 39,\n",
    "      \"mother_race\": \"1\",\n",
    "      \"father_race\": \"1\",\n",
    "      \"cigarette_use\": \"False\",\n",
    "      \"mother_married\": \"True\",\n",
    "      \"ever_born\": \"1\"  \n",
    "    },\n",
    "      {\n",
    "        'is_male': 'Unknown',\n",
    "        'mother_age': 29.0,\n",
    "        'plurality': 'Multiple(2+)',\n",
    "        'gestation_weeks': 38,\n",
    "        \"mother_race\": \"1\",\n",
    "        \"father_race\": \"1\",\n",
    "      \"cigarette_use\": \"False\",\n",
    "      \"mother_married\": \"True\",\n",
    "      \"ever_born\": \"1\"           \n",
    "      }\n",
    "  ]\n",
    "}\n",
    "parent = \"projects/{}/models/{}\".format(PROJECT, MODEL_NAME) # use default version\n",
    "#parent = \"projects/{}/models/{}/versions/{}\".format(PROJECT,VERSION) # specify a specific version\n",
    "\n",
    "response = api.projects().predict(body = data, name = parent).execute()\n",
    "print(\"response = {0}\".format(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model for batch prediction\n",
    "\n",
    "Batch prediction is commonly used when you want to make thousands to millions of predictions at a time. To perform batch prediction we'll create a file with one instance per line and submit the entire prediction job through a `gcloud` command.\n",
    "\n",
    "To illustrate this, let's create a file `inputs.json` which has two instances on which we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\"is_male\": \"True\", \"mother_age\": 26.0, \"plurality\": \"Triplets(3)\", \"gestation_weeks\": 39, \"mother_race\": \"1\", \"father_race\": \"1\", \"cigarette_use\": \"False\", \"mother_married\": \"True\", \"ever_born\": \"1\"}\n",
    "{\"is_male\": \"True\",\"mother_age\": 26.0, \"plurality\": \"Single(1)\", \"gestation_weeks\": 39, \"mother_race\": \"1\", \"father_race\": \"1\", \"cigarette_use\": \"False\", \"mother_married\": \"True\", \"ever_born\": \"1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**\n",
    "\n",
    "In the cells below we'll write the `inputs.json` file we just created to our Cloud storage bucket, then submit a batch prediction job to the cloud pointing at that file. We'll also need to specify the output location in GCS where we'd like the final predicitons to be deposited. In the TODOs below, you will need to \n",
    "- Use `gsutil` to copy the `inputs.json` file to the location specified by `INPUT`\n",
    "- Use `gsutil` to clear out the directory specified by `OUTPUT`. This will ensure the only thing is that location are our predictions\n",
    "- Complete the `glcoud` command to submit a batch prediction job. \n",
    "- Specify the values of all the arguments for the `gcloud` command\n",
    "\n",
    "Have a look at the documentation for submitting batch predictions via `gcloud` to remind yourself of the format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://inputs.json [Content-Type=application/json]...\n",
      "/ [1 files][  198.0 B/  198.0 B]                                                \n",
      "Operation completed over 1 objects/198.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "INPUT=gs://${BUCKET}/babyweight/batchpred/inputs.json\n",
    "OUTPUT=gs://${BUCKET}/babyweight/batchpred/outputs\n",
    "\n",
    "gsutil cp inputs.json $INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\n",
      "[7.630661487579346]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict --model=\"babyweight\" --json-instances=\"inputs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: babyweights_190723_192707\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [babyweights_190723_192707] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweights_190723_192707\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweights_190723_192707\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "INPUT=gs://${BUCKET}/babyweight/batchpred/inputs.json\n",
    "OUTPUT=gs://${BUCKET}/babyweight/batchpred/outputs\n",
    "    \n",
    "gcloud ai-platform jobs submit prediction babyweights_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --data-format=TEXT \\\n",
    "    --region ${REGION} \\\n",
    "    --input-paths=$INPUT \\\n",
    "    --output-path=$OUTPUT \\\n",
    "    --model=\"babyweight\" \\\n",
    "    --version=${MODEL_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the [ML Engine jobs submitted to the GCP console](https://pantheon.corp.google.com/mlengine/jobs) to make sure the prediction job has completed, then let's have a look at the results of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/batchpred/outputs/prediction.errors_stats-00000-of-00001\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/batchpred/outputs/prediction.results-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/babyweight/batchpred/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [7.630660533905029]}\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://$BUCKET/babyweight/batchpred/outputs/prediction.results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
