{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Cloud ML Engine\n",
    "\n",
    "**Learning Objectives**\n",
    "- Use CMLE to run a distributed training job\n",
    "\n",
    "## Introduction \n",
    "After having testing our training pipeline both locally and in the cloud on a susbset of the data, we can submit another (much larger) training job to the cloud. It is also a good idea to run a hyperparameter tuning job to make sure we have optimized the hyperparameters of our model. \n",
    "\n",
    "This notebook illustrates how to do distributed training and hyperparameter tuning on Cloud ML Engine. \n",
    "\n",
    "To start, we'll set up our environment variables as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-636667ae83e902b6\"  # Replace with your PROJECT\n",
    "BUCKET =  \"qwiklabs-gcp-636667ae83e902b6_al\"  # Replace with your BUCKET\n",
    "REGION = \"us-east1\"            # Choose an available region for AI Platform  \n",
    "TFVERSION = \"1.13\"                # TF version for AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look for the preprocessed data for the babyweight model and copy it over if it's not there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls -r gs://$BUCKET | grep -q gs://$BUCKET/babyweight/preproc; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    # copy canonical set of preprocessed files if you didn't do previous notebook\n",
    "    gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/eval.csv-00000-of-00013\n",
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/train.csv-00000-of-00188\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weight_pounds,is_male,mother_age,mother_race,father_race,cigarette_use,mother_married,ever_born,plurality,weight_gain_pounds,gestation_weeks'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"weight_pounds,is_male,mother_age,mother_race,father_race,cigarette_use,mother_married,ever_born,plurality,weight_gain_pounds,gestation_weeks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure I have the extra fields I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.6452239625,Unknown,30,1,1,false,true,2,Single(1),45,38\n",
      "9.6452239625,false,30,1,1,false,true,2,Single(1),45,38\n",
      "5.81358984894,Unknown,30,1,1,false,true,2,Single(1),30,38\n",
      "5.81358984894,false,30,1,1,false,true,2,Single(1),30,38\n",
      "3.5163730789,Unknown,30,1,1,false,true,2,Multiple(2+),33,38\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cat gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/eval.csv-00000-of-00013 | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.09668021378,Unknown,18,1,99,None,false,2,Single(1),99,40\n",
      "7.09668021378,false,18,1,99,None,false,2,Single(1),99,40\n",
      "8.6751900097,Unknown,42,1,99,false,false,1,Single(1),25,41\n",
      "8.6751900097,false,42,1,99,false,false,1,Single(1),25,41\n",
      "7.9917569975,Unknown,22,1,99,false,false,2,Single(1),40,39\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cat gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/train.csv-00000-of-00188 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous labs we developed our TensorFlow model and got it working on a subset of the data. Now we can package the TensorFlow code up as a Python module and train it on Cloud ML Engine.\n",
    "\n",
    "## Train on Cloud ML Engine\n",
    "\n",
    "Training on Cloud ML Engine requires two things:\n",
    "- Configuring our code as a Python package\n",
    "- Using gcloud to submit the training code to Cloud ML Engine\n",
    "\n",
    "### Move code into a Python package\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices.\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location, the directory `babyweight` should already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `%%writefile` magic to write the contents of the cell below to a file called `task.py` in the `babyweight/trainer` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "The cell below write the file `babyweight/trainer/task.py` which sets up our training job. Here is where we determine which parameters of our model to pass as flags during training using the `parser` module. Look at how `batch_size` is passed to the model in the code below. Use this as an example to parse arguements for the following variables\n",
    "- `nnsize` which represents the hidden layer sizes to use for DNN feature columns\n",
    "- `nembeds` which represents the embedding size of a cross of n key real-valued parameters\n",
    "- `train_examples` which represents the number of examples (in thousands) to run the training job\n",
    "- `eval_steps` which represents the positive number of steps for which to evaluate model\n",
    "- `pattern` which specifies a pattern that has to be in input files. For example '00001-of' would process only one shard. For this variable, set 'of' to be the default. \n",
    "\n",
    "Be sure to include a default value for the parsed arguments above and specfy the `type` if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--bucket\",\n",
    "        help = \"GCS path to data. We assume that data is in \\\n",
    "        gs://BUCKET/babyweight/preproc/\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Number of examples to compute gradient over.\",\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help = \"Hidden layer sizes to use for DNN (string, comma-separated)\",\n",
    "        default=\"[10,10]\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help = \"Embedding size of a cross of n key parameters - this will be a small integer\",\n",
    "        default = 3)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--ntrees\",\n",
    "        help = \"Number of trees\",\n",
    "        default = 100)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--maxdepth\",\n",
    "        help = \"Depth of trees\",\n",
    "        default = 6)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"Number of examples (in thousands) to run the training job\",\n",
    "        default=1)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Steps for which to evaluate model\",\n",
    "        default=100)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--pattern\",\n",
    "        help = \"Pattern that appears in filename\",\n",
    "        default = \"of\")\n",
    "        \n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Pop unnecessary args needed for gcloud\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Assign the arguments to the model variables\n",
    "    output_dir = arguments.pop(\"output_dir\")\n",
    "    model.BUCKET     = arguments.pop(\"bucket\")\n",
    "    model.BATCH_SIZE = int(arguments.pop(\"batch_size\"))\n",
    "    model.TRAIN_STEPS = 200000 \n",
    "    #model.TRAIN_STEPS = (int(arguments.pop(\"train_examples\") * 1000)) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop(\"eval_steps\")    \n",
    "\n",
    "    #print (\"Will train for {} steps using batch_size={}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop(\"pattern\")\n",
    "    model.NEMBEDS= arguments.pop(\"nembeds\")\n",
    "    model.NNSIZE = arguments.pop(\"nnsize\")\n",
    "    #print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
    "  \n",
    "    model.MAXDEPTH = int(arguments.pop(\"maxdepth\")) \n",
    "    model.NTREES = int(arguments.pop(\"ntrees\"))\n",
    "    \n",
    "    print (\"Will train on {} trees with max depth of {}\".format(model.NTREES, model.MAXDEPTH))\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate_gbt(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can write to the file `model.py` the model that we developed in the previous notebooks. \n",
    "\n",
    "#### **Exercise 2**\n",
    "\n",
    "Complete the TODOs in the code cell below to create out `model.py`. We'll use the code we wrote for the Wide & Deep model. Look back at your `3_tensorflow_wide_deep` notebook and copy/paste the necessary code from that notebook into its place in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/model.py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BUCKET = None  # set from task.py\n",
    "PATTERN = \"of\" # gets all files\n",
    "\n",
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,father_race,cigarette_use,mother_married,ever_born,plurality,weight_gain_pounds,gestation_weeks'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "CSV_DEFAULTS = [[0.0], ['Unknown'], [0], ['0'], ['0'], ['False'], ['True'], ['1'], ['Single(1)'], [30], [0]]\n",
    "\n",
    "# Define some hyperparameters\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 512\n",
    "NEMBEDS = 3\n",
    "NNSIZE = [64, 16, 4]\n",
    "#NTREES = 100\n",
    "#MAXDEPTH = 6\n",
    "\n",
    "\n",
    "def decode_csv(line_of_text):\n",
    "    fields = tf.decode_csv(records = line_of_text, record_defaults = CSV_DEFAULTS, na_value='None')\n",
    "    features = dict(zip(CSV_COLUMNS, fields))\n",
    "    features['mother_race'] = tf.cast(features['mother_race'], 'string')\n",
    "    features['father_race'] = tf.cast(features['father_race'], 'string')\n",
    "    features['plurality'] = tf.cast(features['plurality'], 'string')\n",
    "    features['weight_gain_pounds'] = tf.cast(features['weight_gain_pounds'], 'int32')\n",
    "    if (features['weight_gain_pounds'] == 99):\n",
    "        features['weight_gain_pounds'] = 30\n",
    "    label = features.pop(LABEL_COLUMN) # remove label from features and store\n",
    "    return features, label\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename_pattern, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "    \n",
    "        path_to_files = 'gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/'\n",
    "        # Create list of files that match pattern.  Does support internal wildcarding e.g. \"babyweight*.csv\"\n",
    "        file_list = tf.gfile.Glob(path_to_files + filename_pattern + \"*\" + PATTERN + \"*\")\n",
    "\n",
    "        print(filename_pattern)\n",
    "        print(file_list)\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list).skip(count = 1)\n",
    "        dataset = dataset.map(map_func = decode_csv)\n",
    "\n",
    "        # In training mode, shuffle the dataset and repeat indefinitely\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "            num_epochs = None \n",
    "        else:\n",
    "            num_epochs = 1 \n",
    "\n",
    "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "        return dataset\n",
    "\n",
    "        # This will now return batches of features, label\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "# Define feature columns\n",
    "def get_categorical(name, values):\n",
    "    return tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(key=name, vocabulary_list=values))\n",
    "\n",
    "    \n",
    "num_cols = ['mother_age', 'gestation_weeks', 'weight_gain_pounds']\n",
    "cat_cols = ['is_male', 'mother_race', 'father_race', 'cigarette_use', 'mother_married', 'plurality', 'ever_born']\n",
    "\n",
    "\n",
    "cat_vocab = {\n",
    "            'is_male': ['True', 'False', 'Unknown'], \n",
    "             'cigarette_use': ['True', 'False', 'None'], \n",
    "             'mother_married': ['True', 'False'], \n",
    "             'mother_race': ['1', '7', '2', '0', '3', '18', '28', '5', '48', '4', '68', '9', '78',\n",
    "        '6', '38', '58'], \n",
    "             'father_race': ['1', '7', '2', '0', '3', '18', '28', '5', '48', '4', '68', '9', '78',\n",
    "        '6', '38', '58'], \n",
    "             'plurality': ['Single(1)', 'Twins(2)', 'Multiple(2+)', 'Triplets(3)',\n",
    "       'Quintuplets(5)', 'Quadruplets(4)'] ,\n",
    "              'ever_born': ['1', '2', '3', '4', '5']\n",
    "            }\n",
    "\n",
    "def get_cols(num_cols, cat_cols, cat_vocab):\n",
    "    all_cols = []\n",
    "    for col in num_cols:\n",
    "        all_cols.append(tf.feature_column.numeric_column(key = col))\n",
    "    for col in cat_cols:\n",
    "        all_cols.append(get_categorical(col, cat_vocab[col]))\n",
    "\n",
    "    #fc_crossed_race = tf.feature_column.crossed_column(keys = ['mother_race', 'father_race'], hash_bucket_size = 100)\n",
    "    \n",
    "    #all_cols.append(tf.feature_column.indicator_column(categorical_column = fc_crossed_race))\n",
    "    return all_cols\n",
    "\n",
    "\n",
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    num_placeholders = {col: tf.placeholder(dtype=tf.float32, shape=[None], name=col) for col in num_cols}     \n",
    "    cat_placeholders = {col: tf.placeholder(dtype=tf.string, shape=[None], name=col) for col in cat_cols}\n",
    "    \n",
    "    feature_placeholders = {**num_placeholders, **cat_placeholders}\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(input = tensor, axis = -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)\n",
    "\n",
    "# create metric for hyperparameter tuning\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = predictions[\"predictions\"]\n",
    "    return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "\n",
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate_dnn(output_dir):\n",
    "    EVAL_INTERVAL = 300\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = EVAL_INTERVAL,\n",
    "        tf_random_seed=42,\n",
    "        keep_checkpoint_max = 3)\n",
    "\n",
    "    estimator = tf.estimator.DNNRegressor(model_dir=output_dir,\n",
    "                                         feature_columns = get_cols(num_cols, cat_cols, cat_vocab),\n",
    "                                         hidden_units = [64,32],\n",
    "                                         config=run_config)\n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = read_dataset(\"train\", mode = tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = read_dataset(\"eval\", mode=tf.estimator.ModeKeys.EVAL), exporters=exporter)\n",
    "        \n",
    "    tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n",
    "    \n",
    "def train_and_evaluate_gbt(output_dir):\n",
    "    EVAL_INTERVAL = 300\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = EVAL_INTERVAL,\n",
    "        tf_random_seed=42,\n",
    "        keep_checkpoint_max = 3)\n",
    "\n",
    "    estimator = tf.estimator.BoostedTreesRegressor(model_dir=output_dir,\n",
    "                                                   n_batches_per_layer = 1,\n",
    "                                         feature_columns = get_cols(num_cols, cat_cols, cat_vocab),\n",
    "                                         n_trees=NTREES,\n",
    "                                         max_depth=MAXDEPTH,   \n",
    "                                         learning_rate=0.05,          \n",
    "                                         config=run_config)\n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = read_dataset(\"train\", mode = tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.BestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = read_dataset(\"eval\", mode=tf.estimator.ModeKeys.EVAL), exporters=exporter)\n",
    "                  \n",
    "    tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train locally\n",
    "\n",
    "After moving the code to a package, make sure it works as a standalone. Note, we incorporated the `--pattern` and `--train_examples` flags so that we don't try to train on the entire dataset while we are developing our pipeline. Once we are sure that everything is working on a subset, we can change the pattern so that we can train on all the data. Even for this subset, this takes about *3 minutes* in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**\n",
    "\n",
    "Fill in the missing code in the TODOs below so that we can run a very small training job over a single file (i.e. use the `pattern` equal to \"00000-of-\") with 1 train step and 1 eval step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=qwiklabs-gcp-636667ae83e902b6_al\n",
      "Will train on 100 trees with max depth of 6\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "train\n",
      "['gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/train.csv-00000-of-00188']\n",
      "eval\n",
      "['gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/preproc/eval.csv-00000-of-00013']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_tf_random_seed': 42, '_model_dir': 'babyweight_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 3, '_service': None, '_evaluation_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_task_id': 0, '_log_step_count_steps': 100, '_num_worker_replicas': 1, '_eval_distribute': None, '_experimental_distribute': None, '_task_type': 'worker', '_save_checkpoints_secs': 300, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe330c4f908>, '_master': '', '_global_id_in_cluster': 0, '_device_fn': None, '_save_summary_steps': 100}\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:256: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': 42, '_model_dir': 'babyweight_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 3, '_service': None, '_evaluation_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_task_id': 0, '_log_step_count_steps': 100, '_num_worker_replicas': 1, '_eval_distribute': None, '_experimental_distribute': None, '_task_type': 'worker', '_save_checkpoints_secs': 300, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe330c4fcc0>, '_master': '', '_global_id_in_cluster': 0, '_device_fn': None, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: VocabularyListCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyListCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:117: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-07-26 12:59:16.797951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-07-26 12:59:16.807499: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-07-26 12:59:16.807924: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5635f84c1f40 executing computations on platform Host. Devices:\n",
      "2019-07-26 12:59:16.807959: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 54.075745, step = 0\n",
      "INFO:tensorflow:global_step/sec: 23.6023\n",
      "INFO:tensorflow:loss = 1.1225827, step = 100 (4.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.882\n",
      "INFO:tensorflow:loss = 1.1461722, step = 200 (4.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.4405\n",
      "INFO:tensorflow:loss = 1.3021522, step = 300 (4.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.789\n",
      "INFO:tensorflow:loss = 0.92333865, step = 400 (4.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.4882\n",
      "INFO:tensorflow:loss = 0.9759283, step = 500 (4.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.3888\n",
      "INFO:tensorflow:loss = 1.073205, step = 600 (4.905 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into babyweight_trained/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-26T12:59:46Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-600\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-26-12:59:51\n",
      "INFO:tensorflow:Saving dict for global step 600: average_loss = 1.0497097, global_step = 600, label/mean = 7.438621, loss = 1.0497097, prediction/mean = 7.3453197, rmse = 1.0245534\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: babyweight_trained/model.ckpt-600\n",
      "INFO:tensorflow:Loading best metric from event files.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Loss for final step: 1.073205.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf babyweight_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "/usr/bin/python3.5 -m trainer.task \\\n",
    "    --bucket=$BUCKET \\\n",
    "    --output_dir=babyweight_trained \\\n",
    "    --job-dir=./tmp \\\n",
    "    --pattern=\"00000-of-\"\\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "The JSON below represents an input into your prediction model. Write the input.json file below with the next cell, then run the prediction locally to assess whether it produces predictions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\"is_male\": \"True\",  \"mother_age\": 26.0,  \"mother_race\": \"1\",  \"father_race\": \"1\", \"cigarette_use\": \"False\", \"mother_married\": \"True\", \"ever_born\": \"1\", \"plurality\": \"Single(1)\", \"gestation_weeks\": 39, \"weight_gain_pounds\": 30}\n",
    "{\"is_male\": \"False\",  \"mother_age\": 26.0,  \"mother_race\": \"1\",  \"father_race\": \"1\", \"cigarette_use\": \"False\", \"mother_married\": \"True\", \"ever_born\": \"1\", \"plurality\": \"Single(1)\", \"gestation_weeks\": 39, \"weight_gain_pounds\": 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 4**\n",
    "\n",
    "Finish the code in cell below to run a local prediction job on the `inputs.json` file we just created. You will need to provide two additional flags\n",
    "- one for `model-dir` specifying the location of the model binaries\n",
    "- one for `json-instances` specifying the location of the json file on which you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/labs/babyweight_trained/export/exporter/*': No such file or directory\n",
      "ERROR: (gcloud.ai-platform.local.predict) Traceback (most recent call last):\n",
      "  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 184, in <module>\n",
      "    main()\n",
      "  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 179, in main\n",
      "    signature_name=args.signature_name)\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 98, in local_predict\n",
      "    client = create_client(framework, model_dir, **kwargs)\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 91, in create_client\n",
      "    return create_client_fn(model_path, **kwargs)\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 522, in create_tf_session_client\n",
      "    return SessionClient(*load_tf_model(model_dir, tags, config))\n",
      "  File \"/usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 217, in load_tf_model\n",
      "    \"Cloud ML only supports TF 1.0 or above and models \"\n",
      "cloud.ml.prediction.prediction_utils.PredictionError: Failed to load model: Cloud ML only supports TF 1.0 or above and models saved in SavedModel format. (Error code: 0)\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\\necho $MODEL_LOCATION\\ngcloud ai-platform local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json\\n'' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-8f356a18793d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\\necho $MODEL_LOCATION\\ngcloud ai-platform local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.5/dist-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\\necho $MODEL_LOCATION\\ngcloud ai-platform local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json\\n'' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "gcloud ai-platform local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Cloud with CMLE\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud ML Engine.  Because this is on the entire dataset, it will take a while. The training run took about <b> an hour </b> for me. You can monitor the job from the GCP console in the Cloud Machine Learning Engine section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### **Exercise 5**\n",
    "\n",
    "Look at the TODOs in the code cell below and fill in the missing information. Some of the required flags are already there for you. You will need to provide the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_gbt_cc us-east1 babyweight_190726_124915\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_gbt_cc\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_gbt_cc us-east1 babyweight_190726_124919\n",
      "jobId: babyweight_190726_124919\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_190726_124919] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_190726_124919\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_190726_124919\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_gbt_cc\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --python-version=3.5 \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --train_examples=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=2000000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN:\n",
    "# Saving dict for global step 200008: \n",
    "# average_loss = 1.037256, global_step = 200008, label/mean = 7.438621, loss = 531.0751, prediction/mean = 7.523035, \n",
    "# rmse = 1.0184577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT:\n",
    "# Saving dict for global step 197872: average_loss = 0.9814496, global_step = 197872, label/mean = 7.438621, \n",
    "# loss = 502.5022, prediction/mean = 7.4479456, rmse = 0.9906814"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Optional: Hyperparameter tuning </h2>\n",
    "<p>\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile.\n",
    "This step will take <b>1 hour</b> -- you can increase maxParallelTrials or reduce maxTrials to get it done faster.  Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 6**\n",
    "\n",
    "We need to create a .yaml file to pass with our hyperparameter tuning job. Fill in the TODOs below for each of the parameters we want to include in our hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE\n",
    "        - parameterName: nembeds\n",
    "          type: INTEGER\n",
    "          minValue: 3\n",
    "          maxValue: 30\n",
    "          scaleType: UNIT_LINEAR_SCALE\n",
    "        - parameterName: nnsize\n",
    "          type: INTEGER\n",
    "          minValue: 64\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run for trees did best at 500 trees which was my max - try more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: ntrees\n",
    "          type: INTEGER\n",
    "          minValue: 400\n",
    "          maxValue: 1000\n",
    "          scaleType: UNIT_LINEAR_SCALE\n",
    "        - parameterName: maxdepth\n",
    "          type: INTEGER\n",
    "          minValue: 2\n",
    "          maxValue: 5\n",
    "          scaleType: UNIT_LINEAR_SCALE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/hyperparam3 us-east1 babyweight_190725_195701\n",
      "jobId: babyweight_190725_195701\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_190725_195701] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_190725_195701\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_190725_195701\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/hyperparam3\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --python-version=3.5 \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --eval_steps=10 \\\n",
    "    --train_examples=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR=\"gs://{}/babyweight/hyperparam3\".format(BUCKET)\n",
    "\n",
    "get_ipython().system_raw(\n",
    "    \"tensorboard --logdir {} --host 0.0.0.0 --port 6006 &\"\n",
    "    .format(OUTDIR))\n",
    "\n",
    "\n",
    "get_ipython().system_raw(\"/home/jupyter/training-data-analyst/courses/machine_learning/asl/02_tensorflow/assets/ngrok http 6006 &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://f674f68d.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Repeat training </h2>\n",
    "<p>\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best parameters:\n",
    "    * batch_size = 56\n",
    "    * nembeds = 21\n",
    "    * nnsize = 180\n",
    "    \n",
    "RMSE was 0.99!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 8, 30, 511 - second-best but the other run gave worse RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For trees the best is 584 trees, depth of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt us-east1 babyweight_190726_000521\n",
      "jobId: babyweight_190726_000521\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/checkpoint#1564099299318799...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/eval/#1564099306714990...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/#1564099296918706...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/events.out.tfevents.1564099179.cmle-training-master-1b12a2b510-0-p6mt6#1564099180415429...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/eval/events.out.tfevents.1564099306.cmle-training-master-1b12a2b510-0-p6mt6#1564099307946883...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-0.index#1564099216962366...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-1168.data-00000-of-00003#1564099298287164...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/events.out.tfevents.1564099183.cmle-training-worker-1b12a2b510-0-rjn2z#1564099184088226...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/graph.pbtxt#1564099213813576...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-0.data-00000-of-00003#1564099216739258...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-0.data-00002-of-00003#1564099216233085...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-1168.data-00001-of-00003#1564099298025427...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-0.data-00001-of-00003#1564099216481135...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-1168.data-00002-of-00003#1564099297764633...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-1168.index#1564099298546647...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-0.meta#1564099218420730...\n",
      "Removing gs://qwiklabs-gcp-636667ae83e902b6_al/babyweight/trained_model_tuned_gbt/model.ckpt-1168.meta#1564099299962388...\n",
      "/ [17/17 objects] 100% Done                                                     \n",
      "Operation completed over 17 objects.                                             \n",
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "Job [babyweight_190726_000521] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_190726_000521\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_190726_000521\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tuned_gbt\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --python-version=3.5 \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --train_examples=2000 --ntrees=584 --maxdepth=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
