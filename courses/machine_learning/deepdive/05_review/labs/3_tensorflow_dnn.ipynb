{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create TensorFlow Deep Neural Network Model\n",
    "\n",
    "**Learning Objective**\n",
    "- Create a DNN model using the high-level Estimator API \n",
    "\n",
    "## Introduction\n",
    "\n",
    "We'll begin by modeling our data using a Deep Neural Network. To achieve this we will use the high-level Estimator API in Tensorflow. Have a look at the various models available through the Estimator API in [the documentation here](https://www.tensorflow.org/api_docs/python/tf/estimator). \n",
    "\n",
    "Start by setting the environment variables related to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-636667ae83e902b6\"  # Replace with your PROJECT\n",
    "BUCKET =  \"qwiklabs-gcp-636667ae83e902b6_al\"  # Replace with your BUCKET\n",
    "REGION = \"us-east1\"            # Choose an available region for AI Platform  \n",
    "TFVERSION = \"1.13\"                # TF version for AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babyweight_test.csv\n",
      "babyweight_train.csv\n",
      "babyweight_valid.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create TensorFlow model using TensorFlow's Estimator API ##\n",
    "\n",
    "We'll begin by writing an input function to read the data and define the csv column names and label column. We'll also set the default csv column values and set the number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have Tensorflow 1.13 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "To begin creating out Tensorflow model, we need to set up variables that determine the csv column values, the label column and the key column. Fill in the TODOs below to set these variables. Note, `CSV_COLUMNS` should be a list and `LABEL_COLUMN` should be a string. It is important to get the column names in the correct order as they appear in the csv train/eval/test sets. If necessary, look back at the previous notebooks at how these csv files were created to ensure you have the correct ordering. \n",
    "\n",
    "We also need to set `DEFAULTS` for each of the CSV column values we prescribe. This will also the a list of entities that will vary depending on the data type of the csv column value. Have a look back at the previous examples to ensure you have the proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_pounds,is_male,mother_age,mother_race,father_race,cigarette_use,mother_married,ever_born,plurality,gestation_weeks,had_ultrasound\n",
      "6.6689834255,Unknown,30,2.0,2.0,False,True,2.0,Single(1),39.0,False\n",
      "6.75055446244,True,22,2.0,9.0,False,False,1.0,Single(1),40.0,True\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -3 babyweight_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,father_race,cigarette_use,mother_married,ever_born,plurality,gestation_weeks,had_ultrasound'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "CSV_DEFAULTS = [[0.0], ['Unknown'], [0.0], ['0.0'], ['0.0'], ['False'], ['True'], [1.0], ['Single(1)'], [0.0], ['False']]\n",
    "TRAIN_STEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the input function\n",
    "\n",
    "Now we are ready to create an input function using the Dataset API.\n",
    "\n",
    "#### **Exercise 2**\n",
    "\n",
    "In the code below you are asked to complete the TODOs to create the input function for our model. Look back at the previous examples we have completed if you need a hint as to how to complete the missing fields below. \n",
    "\n",
    "In the first block of TODOs, your `decode_csv` file should return a dictionary called `features` and a value `label`.\n",
    "\n",
    "In the next TODO, use `tf.gfile.Glob` to create a list of files that match the given `filename_pattern`. Have a look at the documentation for `tf.gfile.Glob` if you get stuck.\n",
    "\n",
    "In the next TODO, use `tf.data.TextLineDataset` to read text file and apply the `decode_csv` function you created above to parse each row example. \n",
    "\n",
    "In the next TODO you are asked to set up the dataset depending on whether you are in `TRAIN` mode or not. (**Hint**: Use `tf.estimator.ModeKeys.TRAIN`). When in `TRAIN` mode, set the appropriate number of epochs and shuffle the data accordingly. When not in `TRAIN` mode, you will use a different number of epochs and there is no need to shuffle the data. \n",
    "\n",
    "Finally, in the last TODO, collect the operations you set up above to produce the final `dataset` we'll use to feed data into our model. \n",
    "\n",
    "Have a look at the examples we did in the previous notebooks if you need inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_csv(line_of_text):\n",
    "    fields = tf.decode_csv(records = line_of_text, record_defaults = CSV_DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, fields))\n",
    "    features['mother_race'] = tf.cast(features['mother_race'], 'string')\n",
    "    features['father_race'] = tf.cast(features['father_race'], 'string')\n",
    "    features['plurality'] = tf.cast(features['plurality'], 'string')\n",
    "    label = features.pop(LABEL_COLUMN) # remove label from features and store\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename_pattern, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "    \n",
    "        # Create list of files that match pattern.  Does support internal wildcarding e.g. \"babyweight*.csv\"\n",
    "        file_list = tf.gfile.Glob(filename_pattern)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list).skip(count = 1)\n",
    "        dataset = dataset.map(map_func = decode_csv)\n",
    "\n",
    "        # In training mode, shuffle the dataset and repeat indefinitely\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "            num_epochs = None \n",
    "        else:\n",
    "            num_epochs = 1 \n",
    "\n",
    "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "        return dataset\n",
    "\n",
    "        # This will now return batches of features, label\n",
    "        return dataset\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature columns\n",
    "\n",
    "Next, we define the feature columns\n",
    "\n",
    "#### **Exercise 3**\n",
    "\n",
    "There are different ways to set up the feature columns for our model. \n",
    "\n",
    "In the first TODO below, you are asked to create a function `get_categorical` which takes a feature name and its potential values and returns an indicator `tf.feature_column` based on a categorical with vocabulary list column. Look back at the documentation for `tf.feature_column.indicator_column` to ensure you call the arguments correctly.\n",
    "\n",
    "In the next TODO, you are asked to complete the code to create a function called `get_cols`. It has no argumnets but should return a list of all the `tf.feature_column`s you intend to use for your model. **Hint**: use the `get_categorical` function you created above to make your code easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical(name, values):\n",
    "    return tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(key=name, vocabulary_list=values))\n",
    "\n",
    "def get_cols(num_cols, cat_cols, cat_vocab):\n",
    "    all_cols = []\n",
    "    for col in num_cols:\n",
    "        all_cols.append(tf.feature_column.numeric_column(key = col))\n",
    "    for col in cat_cols:\n",
    "        all_cols.append(get_categorical(col, cat_vocab[col]))\n",
    "    return all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight_pounds',\n",
       " 'is_male',\n",
       " 'mother_age',\n",
       " 'mother_race',\n",
       " 'father_race',\n",
       " 'cigarette_use',\n",
       " 'mother_married',\n",
       " 'ever_born',\n",
       " 'plurality',\n",
       " 'gestation_weeks',\n",
       " 'had_ultrasound']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this toy dataset \"had_ultrasound\" is a meaningless placeholder, don't try to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['mother_age', 'ever_born', 'gestation_weeks']\n",
    "cat_cols = ['is_male', 'mother_race', 'father_race', 'cigarette_use', 'mother_married', 'plurality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vocab = {\n",
    "            'is_male': ['True', 'False', 'Unknown'], \n",
    "             'cigarette_use': ['True', 'False'], \n",
    "             'mother_married': ['True', 'False'], \n",
    "             'mother_race': [ '1.0',  '7.0',  '2.0',  '0.0',  '3.0', '18.0', '28.0',  '5.0', '48.0',  '4.0', '68.0',  '9.0', '78.0',\n",
    "        '6.0', '38.0', '58.0'], \n",
    "             'father_race': [ '1.0',  '7.0',  '2.0',  '0.0',  '3.0', '18.0', '28.0',  '5.0', '48.0',  '4.0', '68.0',  '9.0', '78.0',\n",
    "        '6.0', '38.0', '58.0'], \n",
    "             'plurality': ['Single(1)', 'Twins(2)', 'Multiple(2+)', 'Triplets(3)',\n",
    "       'Quintuplets(5)', 'Quadruplets(4)'] \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Serving Input function \n",
    "\n",
    "To predict with the TensorFlow model, we also need a serving input function. This will allow us to serve prediction later using the predetermined inputs. We will want all the inputs from our user.\n",
    "\n",
    "#### **Exercise 4**\n",
    "In the TODOs below, create the `feature_placeholders` dictionary by setting up the placeholders for each of the features we will use in our model. Look at the documentation for `tf.placeholder` to make sure you provide all the necessary arguments. You'll need to create placeholders for the features\n",
    "- `is_male`\n",
    "- `mother_age`\n",
    "- `plurality`\n",
    "- `gestation_weeks`\n",
    "- `key`\n",
    "\n",
    "You'll also need to create the features dictionary to pass to the `tf.estimator.export.ServingInputReceiver` function. The `features` dictionary will reference the `fearture_placeholders` dict you created above. Remember to expand the dimensions of the tensors you'll incoude in the `features` dictionary to accomodate for batched data we'll send to the model for predicitons later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(cat_cols, num_cols):\n",
    "    num_placeholders = {col: tf.placeholder(dtype=tf.float32, shape=[None], name=col) for col in num_cols}     \n",
    "    cat_placeholders = {col: tf.placeholder(dtype=tf.string, shape=[None], name=col) for col in cat_cols}\n",
    "    \n",
    "    feature_placeholders = {**num_placeholders, **cat_placeholders}\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(input = tensor, axis = -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and run training and evaluation\n",
    "\n",
    "Lastly, we'll create the estimator to train and evaluate. In the cell below, we'll set up a `DNNRegressor` estimator and the train and evaluation operations. \n",
    "\n",
    "#### **Exercise 5**\n",
    "\n",
    "In the cell below, complete the TODOs to create our model for training. \n",
    "- First you must create your estimator using `tf.estimator.DNNRegressor`. \n",
    "- Next, complete the code to set up your `tf.estimator.TrainSpec`, selecting the appropriate input function and dataset to use to read data to your function during training. \n",
    "- Next, set up your `exporter` and `tf.estimator.EvalSpec`.\n",
    "- Finally, pass the variables you created above to call `tf.estimator.train_and_evaluate`\n",
    "\n",
    "Be sure to check the documentation for these Tensorflow operations to make sure you set things up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_dnn(train_data, eval_data, output_dir, num_cols, cat_cols, cat_vocab):\n",
    "    EVAL_INTERVAL = 300\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = EVAL_INTERVAL,\n",
    "        tf_random_seed=42,\n",
    "        keep_checkpoint_max = 3)\n",
    "\n",
    "    estimator = tf.estimator.DNNRegressor(model_dir=output_dir,\n",
    "                                         feature_columns = get_cols(num_cols, cat_cols, cat_vocab),\n",
    "                                         hidden_units = [64,32],\n",
    "                                         config=run_config)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = read_dataset(train_data, mode = tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.BestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn(cat_cols, num_cols))\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = read_dataset(eval_data, mode=tf.estimator.ModeKeys.EVAL))\n",
    "\n",
    "    train_exists = os.path.isfile(train_data)\n",
    "    eval_exists = os.path.isfile(eval_data)\n",
    "    \n",
    "    if not train_exists:\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), train_data)\n",
    "        \n",
    "    if not eval_exists:\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), eval_data)\n",
    "        \n",
    "    tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_keep_checkpoint_max': 3, '_save_checkpoints_secs': 300, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_tf_random_seed': 42, '_train_distribute': None, '_device_fn': None, '_experimental_distribute': None, '_model_dir': 'babyweight_trained_dnn', '_global_id_in_cluster': 0, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa4f6c5f240>, '_master': '', '_is_chief': True, '_service': None, '_eval_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained_dnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 26026.379, step = 1\n",
      "INFO:tensorflow:global_step/sec: 24.4012\n",
      "INFO:tensorflow:loss = 626.4118, step = 101 (4.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.1848\n",
      "INFO:tensorflow:loss = 545.85034, step = 201 (3.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.6249\n",
      "INFO:tensorflow:loss = 637.64246, step = 301 (3.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.1195\n",
      "INFO:tensorflow:loss = 532.4311, step = 401 (3.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9376\n",
      "INFO:tensorflow:loss = 568.7605, step = 501 (3.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.7505\n",
      "INFO:tensorflow:loss = 582.14984, step = 601 (3.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.0747\n",
      "INFO:tensorflow:loss = 586.8252, step = 701 (3.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.3912\n",
      "INFO:tensorflow:loss = 571.9596, step = 801 (3.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.6077\n",
      "INFO:tensorflow:loss = 641.75366, step = 901 (3.758 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into babyweight_trained_dnn/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-23T13:40:05Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained_dnn/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-23-13:40:08\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.1175313, global_step = 1000, label/mean = 7.3189073, loss = 564.8722, prediction/mean = 7.238484\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: babyweight_trained_dnn/model.ckpt-1000\n",
      "INFO:tensorflow:Loss for final step: 580.6113.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "shutil.rmtree(path = \"babyweight_trained_dnn\", ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate_dnn(\"babyweight_train.csv\", \"babyweight_valid.csv\", \"babyweight_trained_dnn\", num_cols, cat_cols,cat_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Look at the results of your training job above. What RMSE (`average_loss`) did you get for the final eval step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright 2017-2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_gbt(train_data, eval_data, output_dir, num_cols, cat_cols, cat_vocab):\n",
    "    EVAL_INTERVAL = 300\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = EVAL_INTERVAL,\n",
    "        tf_random_seed=42,\n",
    "        keep_checkpoint_max = 3)\n",
    "\n",
    "    estimator = tf.estimator.BoostedTreesRegressor(model_dir=output_dir,\n",
    "                                                   n_batches_per_layer = 1,\n",
    "                                         feature_columns = get_cols(num_cols, cat_cols, cat_vocab),\n",
    "                                         n_trees=50,\n",
    "                                         max_depth=6,   \n",
    "                                         learning_rate=0.05,          \n",
    "                                         config=run_config)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = read_dataset(train_data, mode = tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps = TRAIN_STEPS)\n",
    "    \n",
    "    exporter = tf.estimator.BestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn(cat_cols, num_cols))\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = read_dataset(eval_data, mode=tf.estimator.ModeKeys.EVAL))\n",
    "\n",
    "    train_exists = os.path.isfile(train_data)\n",
    "    eval_exists = os.path.isfile(eval_data)\n",
    "    \n",
    "    if not train_exists:\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), train_data)\n",
    "        \n",
    "    if not eval_exists:\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), eval_data)\n",
    "                  \n",
    "    tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_keep_checkpoint_max': 3, '_save_checkpoints_secs': 300, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_tf_random_seed': 42, '_train_distribute': None, '_device_fn': None, '_experimental_distribute': None, '_model_dir': 'babyweight_trained_gbt', '_global_id_in_cluster': 0, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa4c9789400>, '_master': '', '_is_chief': True, '_service': None, '_eval_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_task_type': 'worker', '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained_gbt/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 54.73403, step = 0\n",
      "INFO:tensorflow:global_step/sec: 24.4647\n",
      "INFO:tensorflow:loss = 1.0168068, step = 100 (4.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.0199\n",
      "INFO:tensorflow:loss = 0.96103436, step = 200 (4.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.6639\n",
      "INFO:tensorflow:loss = 1.0467665, step = 300 (4.616 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into babyweight_trained_gbt/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-23T13:41:27Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained_gbt/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-23-13:41:30\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 1.0745825, global_step = 300, label/mean = 7.3189073, loss = 1.0750755, prediction/mean = 7.289536\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: babyweight_trained_gbt/model.ckpt-300\n",
      "INFO:tensorflow:Loss for final step: 1.0467665.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(path = \"babyweight_trained_gbt\", ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate_gbt(\"babyweight_train.csv\", \"babyweight_valid.csv\", \"babyweight_trained_gbt\", num_cols, cat_cols,cat_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE for DNN: 1.06\n",
      "Final RMSE for GBT: 1.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Final RMSE for DNN: %s\" % round(math.sqrt(1.12), 2))\n",
    "print(\"Final RMSE for GBT: %s\" % round(math.sqrt(1.05), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
