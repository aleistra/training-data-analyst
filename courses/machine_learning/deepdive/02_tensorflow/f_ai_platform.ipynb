{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing AI Platform Training Service\n",
    "**Learning Objectives:**\n",
    "  - Learn how to make code compatible with AI Platform Training Service\n",
    "  - Train your model using cloud infrastructure via AI Platform Training Service\n",
    "  - Deploy your model behind a production grade REST API using AI Platform Training Service\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we'll make the jump from training and predicting locally, to do doing both in the cloud. We'll take advantage of Google Cloud's [AI Platform Training Service](https://cloud.google.com/ai-platform/). \n",
    "\n",
    "AI Platform Training Service is a managed service that allows the training and deployment of ML models without having to provision or maintain servers. The infrastructure is handled seamlessly by the managed service for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.debian.org stretch/updates InRelease\n",
      "Ign:2 http://deb.debian.org/debian stretch InRelease\n",
      "Get:4 http://deb.debian.org/debian stretch-updates InRelease [91.0 kB]\n",
      "Hit:5 http://packages.cloud.google.com/apt cloud-sdk-stretch InRelease\n",
      "Hit:6 https://nvidia.github.io/libnvidia-container/debian9/amd64  InRelease\n",
      "Hit:7 https://nvidia.github.io/nvidia-container-runtime/debian9/amd64  InRelease\n",
      "Hit:8 https://nvidia.github.io/nvidia-docker/debian9/amd64  InRelease\n",
      "Get:9 http://deb.debian.org/debian stretch-backports InRelease [91.8 kB]\n",
      "Hit:10 https://deb.nodesource.com/node_11.x stretch InRelease\n",
      "Hit:11 http://packages.cloud.google.com/apt google-compute-engine-stretch-stable InRelease\n",
      "Hit:12 http://deb.debian.org/debian stretch Release\n",
      "Get:13 https://download.docker.com/linux/debian stretch InRelease [44.8 kB]\n",
      "Hit:3 https://packages.cloud.google.com/apt kubernetes-xenial InRelease\n",
      "Hit:14 http://packages.cloud.google.com/apt google-cloud-packages-archive-keyring-stretch InRelease\n",
      "Get:15 http://deb.debian.org/debian stretch-backports/main amd64 Packages.diff/Index [27.8 kB]\n",
      "Get:16 http://deb.debian.org/debian stretch-backports/main amd64 Packages 2019-07-17-2020.08.pdiff [19.3 kB]\n",
      "Get:16 http://deb.debian.org/debian stretch-backports/main amd64 Packages 2019-07-17-2020.08.pdiff [19.3 kB]\n",
      "Get:18 https://download.docker.com/linux/debian stretch/stable amd64 Packages [9,307 B]\n",
      "Fetched 284 kB in 1s (188 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "google-cloud-sdk is already the newest version (254.0.0-0).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have the latest version of gcloud installed\n",
    "!sudo apt-get update && sudo apt-get --only-upgrade install google-cloud-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with AI Platform Training Service\n",
    "In order to make our code compatible with AI Platform Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a Python package\n",
    "3. Modify code to read data from and write checkpoint files to GCS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Google Cloud Storage (GCS)\n",
    "\n",
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS.\n",
    "\n",
    "Specify your project name and bucket name in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"cloud-training-demos\"  # Replace with your PROJECT\n",
    "BUCKET = \"cloud-training-bucket\"  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for AI Platform Training Service\n",
    "TFVERSION = \"1.13\"                # TF version for AI Platform Training Service to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter allows the subsitution of python variables into bash commands when using the `!<cmd>` format.\n",
    "It is also possible using the `%%bash` magic but requires an [additional parameter](https://stackoverflow.com/questions/19579546/can-i-access-python-variables-within-a-bash-or-script-ipython-notebook-c). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [cloud-training-demos] or it does not exist.\n",
      "Creating gs://cloud-training-bucket/...\n",
      "AccessDeniedException: 403 1088271623242-compute@developer.gserviceaccount.com does not have storage.buckets.create access to project 663413318684.\n",
      "CommandException: No URLs matched: *.csv\n",
      "CommandException: 1 file/object could not be transferred.\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {PROJECT}\n",
    "!gsutil mb -l {REGION} gs://{BUCKET}\n",
    "!gsutil -m cp *.csv gs://{BUCKET}/taxifare/smallinput/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move code into a python package\n",
    "\n",
    "When you execute a AI Platform Training Service training job, the service zips up your code and ships it to the Cloud so it can be run on Cloud infrastructure. In order to do this AI Platform Training Service requires your code to be a Python package.\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create package directory and \\_\\_init\\_\\_.py\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir taxifaremodel\n",
    "touch taxifaremodel/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paste existing code into model.py\n",
    "\n",
    "A Python package requires our code to be in a .py file, as opposed to notebook cells. So, we simply copy and paste our existing code for the previous notebook into a single file.\n",
    "\n",
    "The %%writefile magic writes the contents of its cell to disk with the specified name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write the content of the`model.py` packaging the model we \n",
    "developped in the previous labs so that we can deploy it to AI Platform Training Service. \n",
    "\n",
    "You'll need to reuse the input functions, the `EvalSpec`, `TrainSpec`, `RunConfig`, etc.\n",
    "we implemented in the previous labs.\n",
    "\n",
    "Run the two cell below this one to test your code (the one that creates the `task.py` and the following one that launches a local training).\n",
    "\n",
    "When your code runs locally, execute the next cells to train and deploy your packaged model to AI Platform Training Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxifaremodel/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/model.py\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7]]\n",
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:]\n",
    "\n",
    "def parse_row(row):\n",
    "    fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "    label = features.pop(\"fare_amount\")\n",
    "    return features, label\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filenames = filename).skip(count = 1))\n",
    "    dataset = dataset.map(map_func = parse_row)\n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    dataset = read_dataset(csv_path)\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    dataset = read_dataset(csv_path)\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    return dataset\n",
    "  \n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        \"dayofweek\" : tf.placeholder(dtype = tf.int32, shape = [None]), \n",
    "        \"hourofday\" : tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        \"pickuplon\" : tf.placeholder(dtype = tf.float32, shape = [None]), \n",
    "        \"pickuplat\" : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        \"dropofflat\" : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        \"dropofflon\" : tf.placeholder(dtype = tf.float32, shape = [None])\n",
    "    }\n",
    "    \n",
    "    features = receiver_tensors\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)\n",
    "      \n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "    return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "\n",
    "def create_model(model_dir, train_steps):\n",
    "    config = tf.estimator.RunConfig(\n",
    "        tf_random_seed = 1,\n",
    "        save_checkpoints_steps = max(10, train_steps // 10),\n",
    "        model_dir = model_dir\n",
    "    )\n",
    "    \n",
    "    feature_cols = [tf.feature_column.numeric_column(key = k) for k in FEATURE_NAMES]\n",
    "    \n",
    "    model = tf.estimator.DNNRegressor(\n",
    "        hidden_units = [10,10],\n",
    "        feature_columns = feature_cols, \n",
    "        config = config\n",
    "    )\n",
    "    \n",
    "    model = tf.contrib.estimator.add_metrics(model, my_rmse)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    OUTDIR = params[\"output_dir\"]\n",
    "    TRAIN_DATA_PATH = params[\"train_data_path\"]\n",
    "    EVAL_DATA_PATH = params[\"eval_data_path\"]\n",
    "    TRAIN_STEPS = params[\"train_steps\"]\n",
    "\n",
    "    model = create_model(OUTDIR, TRAIN_STEPS)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = lambda: train_input_fn(TRAIN_DATA_PATH),\n",
    "        max_steps = TRAIN_STEPS\n",
    "    )    \n",
    "    exporter = tf.estimator.FinalExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_receiver_fn)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = lambda: eval_input_fn(EVAL_DATA_PATH),\n",
    "        steps = None,\n",
    "        start_delay_secs = 1,\n",
    "        throttle_secs = 1,\n",
    "        exporters = exporter\n",
    "    )\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator = model, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify code to read data from and write checkpoint files to GCS \n",
    "\n",
    "If you look closely above, you'll notice two changes to the code\n",
    "\n",
    "1. The input function now supports reading a list of files matching a file name pattern instead of just a single CSV\n",
    "  - This is useful because large datasets tend to exist in shards.\n",
    "2. The train and evaluate portion is wrapped in a function that takes a parameter dictionary as an argument.\n",
    "  - This is useful because the output directory, data paths and number of train steps will be different depending on whether we're training locally or in the cloud. Parametrizing allows us to use the same code for both.\n",
    "\n",
    "We specify these parameters at run time via the command line. Which means we need to add code to parse command line parameters and invoke `train_and_evaluate()` with those params. This is the job of the `task.py` file. \n",
    "\n",
    "Exposing parameters to the command line also allows us to use AI Platform Training Service's automatic hyperparameter tuning feature which we'll cover in a future lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxifaremodel/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS or local path to training data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help = \"Steps to run the training job for (default: 1000)\",\n",
    "        type = int,\n",
    "        default = 1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help = \"GCS or local path to evaluation data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"This is not used by our model, but it is required by gcloud\",\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using AI Platform Training Service (Local)\n",
    "\n",
    "AI Platform Training Service comes with a local test tool ([`gcloud ai-platform local train`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/local/train)) to ensure we've packaged our code directly. It's best to first run that for a few steps before trying a Cloud job. \n",
    "\n",
    "The arguments before `-- \\` are for AI Platform Training Service\n",
    "- package-path: speficies the location of the Python package\n",
    "- module-name: specifies which `.py` file should be run within the package. `task.py` is our entry point so we specify that\n",
    "\n",
    "The arguments after `-- \\` are sent to our `task.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 13:53:37.409606 139713774593472 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0718 13:53:37.410774 139713774593472 deprecation_wrapper.py:119] From taxifaremodel/model.py:89: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0718 13:53:37.410963 139713774593472 deprecation_wrapper.py:119] From taxifaremodel/model.py:89: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "I0718 13:53:37.411268 139713774593472 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0718 13:53:37.411462 139713774593472 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0718 13:53:37.411737 139713774593472 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.\n",
      "W0718 13:53:37.418400 139713774593472 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0718 13:53:37.453505 139713774593472 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0718 13:53:37.491919 139713774593472 deprecation_wrapper.py:119] From taxifaremodel/model.py:9: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "I0718 13:53:37.526529 139713774593472 estimator.py:1145] Calling model_fn.\n",
      "I0718 13:53:37.526819 139713774593472 estimator.py:1145] Calling model_fn.\n",
      "W0718 13:53:37.532702 139713774593472 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0718 13:53:38.645724 139713774593472 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0718 13:53:38.736262 139713774593472 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py:76: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "I0718 13:53:38.782172 139713774593472 estimator.py:1147] Done calling model_fn.\n",
      "I0718 13:53:38.782412 139713774593472 estimator.py:1147] Done calling model_fn.\n",
      "I0718 13:53:38.782701 139713774593472 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0718 13:53:38.995541 139713774593472 monitored_session.py:240] Graph was finalized.\n",
      "2019-07-18 13:53:38.996037: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-07-18 13:53:39.008148: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-07-18 13:53:39.008942: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c2ed377970 executing computations on platform Host. Devices:\n",
      "2019-07-18 13:53:39.009009: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-07-18 13:53:39.009489: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2019-07-18 13:53:39.062470: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I0718 13:53:39.122890 139713774593472 session_manager.py:500] Running local_init_op.\n",
      "I0718 13:53:39.134788 139713774593472 session_manager.py:502] Done running local_init_op.\n",
      "I0718 13:53:39.453660 139713774593472 basic_session_run_hooks.py:606] Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/02_tensorflow/taxifaremodel/task.py\", line 39, in <module>\n",
      "    model.train_and_evaluate(args)\n",
      "  File \"taxifaremodel/model.py\", line 93, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator = model, train_spec = train_spec, eval_spec = eval_spec)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
      "    return self.run_local()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1480, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n",
      "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n",
      "    _WrappedSession.__init__(self, self._create_session())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n",
      "    return self._sess_creator.create_session()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 878, in create_session\n",
      "    hook.after_create_session(self.tf_sess, self.coord)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/util.py\", line 90, in after_create_session\n",
      "    session.run(self._initializer)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: taxi-train.csv]\n",
      "\t [[node list_files/assert_not_empty/Assert (defined at taxifaremodel/model.py:15) ]]\n",
      "\n",
      "Original stack trace for u'list_files/assert_not_empty/Assert':\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/02_tensorflow/taxifaremodel/task.py\", line 39, in <module>\n",
      "    model.train_and_evaluate(args)\n",
      "  File \"taxifaremodel/model.py\", line 93, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator = model, train_spec = train_spec, eval_spec = eval_spec)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
      "    return self.run_local()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1185, in _train_model_default\n",
      "    input_fn, ModeKeys.TRAIN))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\n",
      "    self._call_input_fn(input_fn, mode))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\n",
      "    return input_fn(**kwargs)\n",
      "  File \"taxifaremodel/model.py\", line 76, in <lambda>\n",
      "    input_fn = lambda: train_input_fn(TRAIN_DATA_PATH),\n",
      "  File \"taxifaremodel/model.py\", line 21, in train_input_fn\n",
      "    dataset = read_dataset(csv_path)\n",
      "  File \"taxifaremodel/model.py\", line 15, in read_dataset\n",
      "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1727, in list_files\n",
      "    return DatasetV1Adapter(DatasetV2.list_files(file_pattern, shuffle, seed))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 761, in list_files\n",
      "    condition, [message], summarize=1, name=\"assert_not_empty\")\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 163, in Assert\n",
      "    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 74, in _assert\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "CPU times: user 180 ms, sys: 76 ms, total: 256 ms\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!gcloud ai-platform local train \\\n",
    "    --package-path=taxifaremodel \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    -- \\\n",
    "    --train_data_path=taxi-train.csv \\\n",
    "    --eval_data_path=taxi-valid.csv  \\\n",
    "    --train_steps=1 \\\n",
    "    --output_dir=taxi_trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using AI Platform Training Service (Cloud)\n",
    "\n",
    "To submit to the Cloud we use [`gcloud ai-platform jobs submit training [jobname]`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) and simply specify some additional parameters for AI Platform Training Service:\n",
    "- jobname: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- job-dir: A GCS location to upload the Python package to\n",
    "- runtime-version: Version of TF to use. Defaults to 1.0 if not specified\n",
    "- python-version: Version of Python to use. Defaults to 2.7 if not specified\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/ml-engine/docs/tensorflow/regions) for supported AI Platform Training Service regions\n",
    "\n",
    "Below the `-- \\` note how we've changed our `task.py` args to be GCS locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"gs://{}/taxifare/trained_small\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.jobs.submit.training) 403 Could not upload file [/tmp/tmpPdxTxc/output/taxifaremodel-0.0.0.tar.gz] to [cloud-training-bucket/taxifare/packages/bbd586e26fb935202b5f48ed1ec86bae53d28da7f56f6a553f576da49dea1c8b/taxifaremodel-0.0.0.tar.gz]: The project to be billed is associated with a closed billing account.\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ai-platform jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=taxifaremodel \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    --job-dir=gs://{BUCKET}/taxifare \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version={TFVERSION} \\\n",
    "    --region={REGION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-train.csv \\\n",
    "    --eval_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-valid.csv  \\\n",
    "    --train_steps=1000 \\\n",
    "    --output_dir={OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your job and view logs using [cloud console](https://console.cloud.google.com/mlengine/jobs). It will take 5-10 minutes to complete. **Wait till the job finishes before moving on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Now let's take our exported SavedModel and deploy it behind a REST API. To do so we'll use AI Platform Training Service's managed TF Serving feature which auto-scales based on load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccessDeniedException: 403 1088271623242-compute@developer.gserviceaccount.com does not have storage.objects.list access to cloud-training-bucket.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{BUCKET}/taxifare/trained_small/export/exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Platform Training Service uses a model versioning system. First you create a model folder, and within the folder you create versions of the model. \n",
    "\n",
    "Note: You will see an error below if the model folder already exists, it is safe to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) User [1088271623242-compute@developer.gserviceaccount.com] does not have permission to access project [cloud-training-demos] (or it may not exist): Access to project denied. This might be a transient error and a retry may succeed. If the error persists, please check the IAM permissions on your project.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.delete) PERMISSION_DENIED: Access to project denied. This might be a transient error and a retry may succeed. If the error persists, please check the IAM permissions on your project.\n",
      "AccessDeniedException: 403 1088271623242-compute@developer.gserviceaccount.com does not have storage.objects.list access to cloud-training-bucket.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.create) argument --origin: expected one argument\n",
      "Usage: gcloud ai-platform versions create VERSION --model=MODEL [optional flags]\n",
      "  optional flags may be  --async | --config | --description | --framework |\n",
      "                         --help | --labels | --origin | --python-version |\n",
      "                         --runtime-version | --staging-bucket\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud ai-platform versions create --help\n"
     ]
    }
   ],
   "source": [
    "VERSION='v1'\n",
    "!gcloud ai-platform models create taxifare --regions us-central1\n",
    "!gcloud ai-platform versions delete {VERSION} --model taxifare --quiet\n",
    "!gcloud ai-platform versions create {VERSION} --model taxifare \\\n",
    "    --origin $(gsutil ls gs://{BUCKET}/taxifare/trained_small/export/exporter | tail -1) \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version {TFVERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online prediction\n",
    "\n",
    "Now that we have deployed our model behind a production grade REST API, we can invoke it remotely. \n",
    "\n",
    "We could invoke it directly calling the REST API with an HTTP POST request [reference docs](https://cloud.google.com/ml-engine/reference/rest/v1/projects/predict), however AI Platform Training Service provides an easy way to invoke it via command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke prediction REST API via command line\n",
    "First we write our prediction requests to file in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test.json\n",
    "{\"dayofweek\": 1, \"hourofday\": 0, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use [`gcloud ai-platform predict`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/predict) and specify the model name and location of the json file. Since we don't explicitly specify `--version`, the default model version will be used. \n",
    "\n",
    "Since we only have one version it is already the default, but if we had multiple model versions we can designate the default using [`gcloud ai-platform versions set-default`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/versions/set-default) or using [cloud console](https://pantheon.corp.google.com/mlengine/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.predict) HTTP request failed. Response: {\n",
      "  \"error\": {\n",
      "    \"code\": 403,\n",
      "    \"message\": \"Access to project denied. This might be a transient error and a retry may succeed. If the error persists, please check the IAM permissions on your project.\",\n",
      "    \"status\": \"PERMISSION_DENIED\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform predict --model=taxifare --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke prediction REST API via python\n",
    "\n",
    "In the cell below, use the Google Python client library to query the model you just deployed on AI Platform Training Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://ml.googleapis.com/v1/projects/cloud-training-demos/models/taxifare:predict?alt=json returned \"Access to project denied. This might be a transient error and a retry may succeed. If the error persists, please check the IAM permissions on your project.\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-996a0f355e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#parent = \"projects/{}/models/taxifare/versions/{}\".format(PROJECT,VERSION) # specify a specific version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response = {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    849\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://ml.googleapis.com/v1/projects/cloud-training-demos/models/taxifare:predict?alt=json returned \"Access to project denied. This might be a transient error and a retry may succeed. If the error persists, please check the IAM permissions on your project.\">"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build(\"ml\", \"v1\", credentials = credentials,\n",
    "            discoveryServiceUrl = \"https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json\")\n",
    "\n",
    "request_data = {\"instances\":\n",
    "  [\n",
    "      {\n",
    "        \"dayofweek\": 1,\n",
    "        \"hourofday\": 8,\n",
    "        \"pickuplon\": -73.885,\n",
    "        \"pickuplat\": 40.773,\n",
    "        \"dropofflon\": -73.987,\n",
    "        \"dropofflat\": 40.732,\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "parent = \"projects/{}/models/taxifare\".format(PROJECT) # use default version\n",
    "#parent = \"projects/{}/models/taxifare/versions/{}\".format(PROJECT,VERSION) # specify a specific version\n",
    "\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {0}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge exercise\n",
    "\n",
    "Modify your solution to the challenge exercise in d_trainandevaluate.ipynb appropriately. Make sure that you implement training and deployment. Increase the size of your dataset by 10x since you are running on the cloud. Does your accuracy improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
